{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRpKk5epp5VhIrxJUo9Q7c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreevanimtcs2502/sreevanimtcs2502/blob/nlp/neg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os, zipfile, urllib.request\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from numpy.linalg import norm\n",
        "import random\n",
        "\n",
        "\n",
        "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
        "if not os.path.exists(\"text8.zip\"):\n",
        "    urllib.request.urlretrieve(url, \"text8.zip\")\n",
        "\n",
        "with zipfile.ZipFile(\"text8.zip\") as z:\n",
        "    text = z.read(\"text8\").decode(\"utf-8\")\n",
        "\n",
        "tokens = text.split()[:5_000_000]   # 5M tokens â†’ safe runtime\n",
        "\n",
        "\n",
        "min_count = 10\n",
        "counts = Counter(tokens)\n",
        "\n",
        "filtered_words = [w for w, c in counts.items() if c >= min_count]\n",
        "vocab = {word: i for i, word in enumerate(filtered_words)}\n",
        "id2word = {i: word for word, i in vocab.items()}\n",
        "\n",
        "\n",
        "original_tokens_for_gensim = list(tokens)\n",
        "\n",
        "\n",
        "t = 1e-5\n",
        "tokens = [\n",
        "    w for w in tokens if w in vocab\n",
        "    and random.random() < np.sqrt(t / counts[w])\n",
        "]\n",
        "\n",
        "VOCAB_SIZE = len(vocab)\n",
        "print(\"Vocab size:\", VOCAB_SIZE)\n",
        "print(\"Tokens after subsampling:\", len(tokens))\n",
        "\n",
        "\n",
        "WINDOW = 5\n",
        "pairs = []\n",
        "\n",
        "for i,w in enumerate(tokens):\n",
        "    target = vocab[w]\n",
        "    start = max(0, i-WINDOW)\n",
        "    end = min(len(tokens), i+WINDOW)\n",
        "    for j in range(start,end):\n",
        "        if i!=j:\n",
        "            pairs.append((target, vocab[tokens[j]]))\n",
        "\n",
        "random.shuffle(pairs)\n",
        "pairs = pairs[:1_000_000]\n",
        "print(\"Training pairs:\", len(pairs))\n",
        "\n",
        "\n",
        "freq = np.array([counts[id2word[i]] for i in range(VOCAB_SIZE)])\n",
        "noise_dist = freq**0.75\n",
        "noise_dist /= noise_dist.sum()\n",
        "\n",
        "\n",
        "EMB = 100\n",
        "NEG = 5\n",
        "LR = 0.025\n",
        "EPOCHS = 1\n",
        "\n",
        "W_in = np.random.uniform(-0.5,0.5,(VOCAB_SIZE,EMB))\n",
        "W_out = np.random.uniform(-0.5,0.5,(VOCAB_SIZE,EMB))\n",
        "\n",
        "def sigmoid(x): return 1/(1+np.exp(-x))\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loss = 0\n",
        "    for t,c in pairs:\n",
        "        v_in = W_in[t]\n",
        "        v_out = W_out[c]\n",
        "\n",
        "\n",
        "        score = sigmoid(np.dot(v_in,v_out))\n",
        "        grad = score - 1\n",
        "\n",
        "        W_out[c] -= LR*grad*v_in\n",
        "        W_in[t] -= LR*grad*v_out\n",
        "\n",
        "\n",
        "        negs = np.random.choice(VOCAB_SIZE,NEG,p=noise_dist)\n",
        "        neg_vecs = W_out[negs]\n",
        "        scores = sigmoid(np.dot(neg_vecs,v_in))\n",
        "\n",
        "        W_out[negs] -= LR*(scores[:,None]*v_in)\n",
        "        W_in[t] -= LR*np.sum(scores[:,None]*neg_vecs,axis=0)\n",
        "\n",
        "        loss += -np.log(score+1e-9)\n",
        "\n",
        "    print(\"Epoch loss:\", round(loss,2))\n",
        "\n",
        "\n",
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "sentences = [original_tokens_for_gensim[i:i+1000] for i in range(0,len(original_tokens_for_gensim),1000)]\n",
        "\n",
        "gensim_model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    negative=5,\n",
        "    sg=1,\n",
        "    min_count=10,\n",
        "    epochs=3,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "\n",
        "def cos(a,b): return np.dot(a,b)/(norm(a)*norm(b))\n",
        "\n",
        "word = \"king\"\n",
        "if word in vocab:\n",
        "    print(\"\\nCosine similarity (king):\",\n",
        "          round(cos(W_in[vocab[word]],gensim_model.wv[word]),3))\n",
        "\n",
        "\n",
        "def analogy(a,b,c):\n",
        "    v = W_in[vocab[a]] - W_in[vocab[b]] + W_in[vocab[c]]\n",
        "    scores = {id2word[i]:cos(v,W_in[i]) for i in range(VOCAB_SIZE)}\n",
        "    return sorted(scores,key=scores.get,reverse=True)[:5]\n",
        "\n",
        "print(\"\\nCustom analogy king-man+woman:\", analogy(\"king\",\"man\",\"woman\"))\n",
        "print(\"Gensim analogy:\",\n",
        "      gensim_model.wv.most_similar(\n",
        "          positive=[\"king\",\"woman\"], negative=[\"man\"], topn=5))\n",
        "\n",
        "\n",
        "bias_custom = W_in[vocab[\"she\"]] - W_in[vocab[\"he\"]]\n",
        "bias_gensim = gensim_model.wv[\"she\"] - gensim_model.wv[\"he\"]\n",
        "\n",
        "def bias_score(word, emb, bias):\n",
        "    return cos(emb[vocab[word]], bias)\n",
        "\n",
        "test_words = [\"doctor\",\"nurse\",\"engineer\",\"teacher\"]\n",
        "\n",
        "print(\"\\nBias scores (custom):\")\n",
        "for w in test_words:\n",
        "    print(w, round(bias_score(w,W_in,bias_custom),3))\n",
        "\n",
        "print(\"\\nBias scores (gensim):\")\n",
        "for w in test_words:\n",
        "    print(w, round(cos(gensim_model.wv[w],bias_gensim),3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70gKsGblJWeX",
        "outputId": "e75beb6e-3c92-4b8f-f598-1dfae7b96d3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 23599\n",
            "Tokens after subsampling: 608\n",
            "Training pairs: 5447\n",
            "Epoch loss: 4190.41\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "\n",
            "Cosine similarity (king): -0.079\n",
            "\n",
            "Custom analogy king-man+woman: ['woman', 'king', 'lowers', 'solids', 'orthography']\n",
            "Gensim analogy: [('throne', 0.6682941317558289), ('amalric', 0.6681479215621948), ('hezekiah', 0.6666582226753235), ('judah', 0.6609217524528503), ('andronicus', 0.6596450805664062)]\n",
            "\n",
            "Bias scores (custom):\n",
            "doctor 0.098\n",
            "nurse -0.143\n",
            "engineer -0.061\n",
            "teacher -0.098\n",
            "\n",
            "Bias scores (gensim):\n",
            "doctor 0.12\n",
            "nurse 0.174\n",
            "engineer -0.031\n",
            "teacher 0.023\n"
          ]
        }
      ]
    }
  ]
}