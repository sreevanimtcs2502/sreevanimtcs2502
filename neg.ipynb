{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZhQxw8dpPwnTFfMzAjjiT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreevanimtcs2502/sreevanimtcs2502/blob/nlp/neg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "EOfT55NTIztg",
        "outputId": "42cf2584-edaf-491a-c93f-0be2b89cbc7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 21681\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "16",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2069679441.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# 4. Negative Sampling Distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# -----------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mword_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mnoise_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mnoise_dist\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnoise_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 16"
          ]
        }
      ],
      "source": [
        "# =======================\n",
        "# Skip-gram with Negative Sampling vs Gensim (Single Cell)\n",
        "# =======================\n",
        "\n",
        "import os, zipfile, urllib.request, re, random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# -----------------------\n",
        "# 1. Download & Load Dataset (text8)\n",
        "# -----------------------\n",
        "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
        "if not os.path.exists(\"text8.zip\"):\n",
        "    urllib.request.urlretrieve(url, \"text8.zip\")\n",
        "\n",
        "with zipfile.ZipFile(\"text8.zip\") as z:\n",
        "    text = z.read(\"text8\").decode(\"utf-8\")\n",
        "\n",
        "tokens = text.split()[:2_000_000]  # medium subset for speed\n",
        "\n",
        "# -----------------------\n",
        "# 2. Build Vocabulary\n",
        "# -----------------------\n",
        "min_count = 5\n",
        "counts = Counter(tokens)\n",
        "vocab = {w:i for i,(w,c) in enumerate(counts.items()) if c>=min_count}\n",
        "id2word = {i:w for w,i in vocab.items()}\n",
        "\n",
        "tokens = [w for w in tokens if w in vocab]\n",
        "\n",
        "VOCAB_SIZE = len(vocab)\n",
        "print(\"Vocab size:\", VOCAB_SIZE)\n",
        "\n",
        "# -----------------------\n",
        "# 3. Generate Training Pairs\n",
        "# -----------------------\n",
        "WINDOW = 5\n",
        "pairs = []\n",
        "\n",
        "for i,w in enumerate(tokens):\n",
        "    target = vocab[w]\n",
        "    for j in range(max(0,i-WINDOW), min(len(tokens),i+WINDOW)):\n",
        "        if i!=j:\n",
        "            pairs.append((target, vocab[tokens[j]]))\n",
        "\n",
        "random.shuffle(pairs)\n",
        "pairs = pairs[:200_000]  # subsample for speed\n",
        "\n",
        "# -----------------------\n",
        "# 4. Negative Sampling Distribution\n",
        "# -----------------------\n",
        "word_freq = np.array([counts[id2word[i]] for i in range(VOCAB_SIZE)])\n",
        "noise_dist = word_freq**0.75\n",
        "noise_dist /= noise_dist.sum()\n",
        "\n",
        "# -----------------------\n",
        "# 5. Skip-gram with Negative Sampling (NumPy)\n",
        "# -----------------------\n",
        "EMB = 100\n",
        "NEG = 5\n",
        "LR = 0.025\n",
        "EPOCHS = 1\n",
        "\n",
        "W_in = np.random.uniform(-0.5,0.5,(VOCAB_SIZE,EMB))\n",
        "W_out = np.random.uniform(-0.5,0.5,(VOCAB_SIZE,EMB))\n",
        "\n",
        "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loss = 0\n",
        "    for t,c in pairs:\n",
        "        z = np.dot(W_in[t], W_out[c])\n",
        "        grad = sigmoid(z)-1\n",
        "\n",
        "        W_out[c] -= LR*grad*W_in[t]\n",
        "        W_in[t] -= LR*grad*W_out[c]\n",
        "\n",
        "        negs = np.random.choice(VOCAB_SIZE,NEG,p=noise_dist)\n",
        "        for n in negs:\n",
        "            zn = np.dot(W_in[t],W_out[n])\n",
        "            gradn = sigmoid(zn)\n",
        "            W_out[n] -= LR*gradn*W_in[t]\n",
        "            W_in[t] -= LR*gradn*W_out[n]\n",
        "\n",
        "        loss += -np.log(sigmoid(z))\n",
        "    print(\"Custom SGNS loss:\", round(loss,2))\n",
        "\n",
        "# -----------------------\n",
        "# 6. Train Gensim Word2Vec\n",
        "# -----------------------\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = [tokens[i:i+1000] for i in range(0,len(tokens),1000)]\n",
        "\n",
        "gensim_model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    negative=5,\n",
        "    sg=1,\n",
        "    min_count=5,\n",
        "    epochs=3\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 7. Cosine Similarity Comparison\n",
        "# -----------------------\n",
        "def cos(a,b): return np.dot(a,b)/(norm(a)*norm(b))\n",
        "\n",
        "word = \"king\"\n",
        "if word in vocab:\n",
        "    print(\"\\nCosine similarity (king):\",\n",
        "          cos(W_in[vocab[word]], gensim_model.wv[word]))\n",
        "\n",
        "# -----------------------\n",
        "# 8. Word Analogy Task\n",
        "# -----------------------\n",
        "def analogy(a,b,c,emb):\n",
        "    v = emb[vocab[a]] - emb[vocab[b]] + emb[vocab[c]]\n",
        "    scores = {id2word[i]:cos(v,emb[i]) for i in range(VOCAB_SIZE)}\n",
        "    return sorted(scores,key=scores.get,reverse=True)[:5]\n",
        "\n",
        "print(\"\\nCustom analogy king-man+woman:\", analogy(\"king\",\"man\",\"woman\",W_in))\n",
        "print(\"Gensim analogy:\",\n",
        "      gensim_model.wv.most_similar(\n",
        "          positive=[\"king\",\"woman\"], negative=[\"man\"], topn=5))\n",
        "\n",
        "# -----------------------\n",
        "# 9. Bias Detection (Gender Bias)\n",
        "# -----------------------\n",
        "bias_custom = W_in[vocab[\"she\"]] - W_in[vocab[\"he\"]]\n",
        "bias_gensim = gensim_model.wv[\"she\"] - gensim_model.wv[\"he\"]\n",
        "\n",
        "def bias_score(word, emb, bias):\n",
        "    return cos(emb[vocab[word]], bias)\n",
        "\n",
        "test_words = [\"doctor\",\"nurse\",\"engineer\",\"teacher\"]\n",
        "\n",
        "print(\"\\nBias scores (custom):\")\n",
        "for w in test_words:\n",
        "    print(w, round(bias_score(w,W_in,bias_custom),3))\n",
        "\n",
        "print(\"\\nBias scores (gensim):\")\n",
        "for w in test_words:\n",
        "    print(w, round(cos(gensim_model.wv[w],bias_gensim),3))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Skip-gram with Negative Sampling (30-min Safe Version)\n",
        "# Dataset: text8 (Wikipedia)\n",
        "# ============================================================\n",
        "\n",
        "import os, zipfile, urllib.request\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from numpy.linalg import norm\n",
        "import random\n",
        "\n",
        "# -----------------------\n",
        "# 1. Download Dataset\n",
        "# -----------------------\n",
        "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
        "if not os.path.exists(\"text8.zip\"):\n",
        "    urllib.request.urlretrieve(url, \"text8.zip\")\n",
        "\n",
        "with zipfile.ZipFile(\"text8.zip\") as z:\n",
        "    text = z.read(\"text8\").decode(\"utf-8\")\n",
        "\n",
        "tokens = text.split()[:5_000_000]   # 5M tokens → safe runtime\n",
        "\n",
        "# -----------------------\n",
        "# 2. Vocabulary + Subsampling\n",
        "# -----------------------\n",
        "min_count = 10\n",
        "counts = Counter(tokens)\n",
        "# Fix: Ensure vocabulary indices are contiguous after filtering by min_count\n",
        "filtered_words = [w for w, c in counts.items() if c >= min_count]\n",
        "vocab = {word: i for i, word in enumerate(filtered_words)}\n",
        "id2word = {i: word for word, i in vocab.items()}\n",
        "\n",
        "# Store original tokens (before subsampling) for Gensim to build its vocabulary\n",
        "original_tokens_for_gensim = list(tokens)\n",
        "\n",
        "# Subsampling frequent words (Mikolov) - this modifies 'tokens' for the custom model\n",
        "t = 1e-5\n",
        "tokens = [\n",
        "    w for w in tokens if w in vocab\n",
        "    and random.random() < np.sqrt(t / counts[w])\n",
        "]\n",
        "\n",
        "VOCAB_SIZE = len(vocab)\n",
        "print(\"Vocab size:\", VOCAB_SIZE)\n",
        "print(\"Tokens after subsampling:\", len(tokens))\n",
        "\n",
        "# -----------------------\n",
        "# 3. Training Pairs\n",
        "# -----------------------\n",
        "WINDOW = 5\n",
        "pairs = []\n",
        "\n",
        "for i,w in enumerate(tokens):\n",
        "    target = vocab[w]\n",
        "    start = max(0, i-WINDOW)\n",
        "    end = min(len(tokens), i+WINDOW)\n",
        "    for j in range(start,end):\n",
        "        if i!=j:\n",
        "            pairs.append((target, vocab[tokens[j]]))\n",
        "\n",
        "random.shuffle(pairs)\n",
        "pairs = pairs[:1_000_000]   # cap pairs → runtime control\n",
        "print(\"Training pairs:\", len(pairs))\n",
        "\n",
        "# -----------------------\n",
        "# 4. Negative Sampling Distribution\n",
        "# -----------------------\n",
        "freq = np.array([counts[id2word[i]] for i in range(VOCAB_SIZE)])\n",
        "noise_dist = freq**0.75\n",
        "noise_dist /= noise_dist.sum()\n",
        "\n",
        "# -----------------------\n",
        "# 5. SGNS Model (Vectorized)\n",
        "# -----------------------\n",
        "EMB = 100\n",
        "NEG = 5\n",
        "LR = 0.025\n",
        "EPOCHS = 1\n",
        "\n",
        "W_in = np.random.uniform(-0.5,0.5,(VOCAB_SIZE,EMB))\n",
        "W_out = np.random.uniform(-0.5,0.5,(VOCAB_SIZE,EMB))\n",
        "\n",
        "def sigmoid(x): return 1/(1+np.exp(-x))\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loss = 0\n",
        "    for t,c in pairs:\n",
        "        v_in = W_in[t]\n",
        "        v_out = W_out[c]\n",
        "\n",
        "        # Positive sample\n",
        "        score = sigmoid(np.dot(v_in,v_out))\n",
        "        grad = score - 1\n",
        "\n",
        "        W_out[c] -= LR*grad*v_in\n",
        "        W_in[t] -= LR*grad*v_out\n",
        "\n",
        "        # Negative samples (vectorized)\n",
        "        negs = np.random.choice(VOCAB_SIZE,NEG,p=noise_dist)\n",
        "        neg_vecs = W_out[negs]\n",
        "        scores = sigmoid(np.dot(neg_vecs,v_in))\n",
        "\n",
        "        W_out[negs] -= LR*(scores[:,None]*v_in)\n",
        "        W_in[t] -= LR*np.sum(scores[:,None]*neg_vecs,axis=0)\n",
        "\n",
        "        loss += -np.log(score+1e-9)\n",
        "\n",
        "    print(\"Epoch loss:\", round(loss,2))\n",
        "\n",
        "# -----------------------\n",
        "# 6. Train Gensim Word2Vec\n",
        "# -----------------------\n",
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Use the original_tokens_for_gensim for sentence creation\n",
        "sentences = [original_tokens_for_gensim[i:i+1000] for i in range(0,len(original_tokens_for_gensim),1000)]\n",
        "\n",
        "gensim_model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    negative=5,\n",
        "    sg=1,\n",
        "    min_count=10,\n",
        "    epochs=3,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 7. Cosine Similarity Comparison\n",
        "# -----------------------\n",
        "def cos(a,b): return np.dot(a,b)/(norm(a)*norm(b))\n",
        "\n",
        "word = \"king\"\n",
        "if word in vocab:\n",
        "    print(\"\\nCosine similarity (king):\",\n",
        "          round(cos(W_in[vocab[word]],gensim_model.wv[word]),3))\n",
        "\n",
        "# -----------------------\n",
        "# 8. Word Analogy\n",
        "# -----------------------\n",
        "def analogy(a,b,c):\n",
        "    v = W_in[vocab[a]] - W_in[vocab[b]] + W_in[vocab[c]]\n",
        "    scores = {id2word[i]:cos(v,W_in[i]) for i in range(VOCAB_SIZE)}\n",
        "    return sorted(scores,key=scores.get,reverse=True)[:5]\n",
        "\n",
        "print(\"\\nCustom analogy king-man+woman:\", analogy(\"king\",\"man\",\"woman\"))\n",
        "print(\"Gensim analogy:\",\n",
        "      gensim_model.wv.most_similar(\n",
        "          positive=[\"king\",\"woman\"], negative=[\"man\"], topn=5))\n",
        "\n",
        "# -----------------------\n",
        "# 9. Bias Detection\n",
        "# -----------------------\n",
        "bias_custom = W_in[vocab[\"she\"]] - W_in[vocab[\"he\"]]\n",
        "bias_gensim = gensim_model.wv[\"she\"] - gensim_model.wv[\"he\"]\n",
        "\n",
        "def bias_score(word, emb, bias):\n",
        "    return cos(emb[vocab[word]], bias)\n",
        "\n",
        "test_words = [\"doctor\",\"nurse\",\"engineer\",\"teacher\"]\n",
        "\n",
        "print(\"\\nBias scores (custom):\")\n",
        "for w in test_words:\n",
        "    print(w, round(bias_score(w,W_in,bias_custom),3))\n",
        "\n",
        "print(\"\\nBias scores (gensim):\")\n",
        "for w in test_words:\n",
        "    print(w, round(cos(gensim_model.wv[w],bias_gensim),3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70gKsGblJWeX",
        "outputId": "e75beb6e-3c92-4b8f-f598-1dfae7b96d3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 23599\n",
            "Tokens after subsampling: 608\n",
            "Training pairs: 5447\n",
            "Epoch loss: 4190.41\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "\n",
            "Cosine similarity (king): -0.079\n",
            "\n",
            "Custom analogy king-man+woman: ['woman', 'king', 'lowers', 'solids', 'orthography']\n",
            "Gensim analogy: [('throne', 0.6682941317558289), ('amalric', 0.6681479215621948), ('hezekiah', 0.6666582226753235), ('judah', 0.6609217524528503), ('andronicus', 0.6596450805664062)]\n",
            "\n",
            "Bias scores (custom):\n",
            "doctor 0.098\n",
            "nurse -0.143\n",
            "engineer -0.061\n",
            "teacher -0.098\n",
            "\n",
            "Bias scores (gensim):\n",
            "doctor 0.12\n",
            "nurse 0.174\n",
            "engineer -0.031\n",
            "teacher 0.023\n"
          ]
        }
      ]
    }
  ]
}