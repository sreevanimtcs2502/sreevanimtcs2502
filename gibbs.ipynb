{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreevanimtcs2502/sreevanimtcs2502/blob/main/gibbs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMt9mj8U8wWe",
        "outputId": "a9ce7215-d61e-499a-8223-c5375f662c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Yelp dataset...\n",
            "Vectorizing text...\n",
            "Running Gibbs Sampling for 100 topics...\n",
            "Iteration 1/20 complete\n",
            "Iteration 2/20 complete\n",
            "Iteration 3/20 complete\n",
            "Iteration 4/20 complete\n",
            "Iteration 5/20 complete\n",
            "Iteration 6/20 complete\n",
            "Iteration 7/20 complete\n",
            "Iteration 8/20 complete\n",
            "Iteration 9/20 complete\n",
            "Iteration 10/20 complete\n",
            "Iteration 11/20 complete\n",
            "Iteration 12/20 complete\n",
            "Iteration 13/20 complete\n",
            "Iteration 14/20 complete\n",
            "Iteration 15/20 complete\n",
            "Iteration 16/20 complete\n",
            "Iteration 17/20 complete\n",
            "Iteration 18/20 complete\n",
            "Iteration 19/20 complete\n",
            "Iteration 20/20 complete\n",
            "\n",
            "âœ… Gibbs sampling complete for 100 topics!\n",
            "\n",
            "Model Accuracy (approx): 0.66\n",
            "\n",
            "Sample Results:\n",
            "\n",
            "Review: dr. goldberg offers everything i look for in a general practitioner.  he's nice ...\n",
            "â†’ True: Positive | Predicted: Positive\n",
            "\n",
            "Review: Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of th...\n",
            "â†’ True: Negative | Predicted: Positive\n",
            "\n",
            "Review: Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patie...\n",
            "â†’ True: Positive | Predicted: Positive\n",
            "\n",
            "Review: Got a letter in the mail last week that said Dr. Goldberg is moving to Arizona t...\n",
            "â†’ True: Positive | Predicted: Negative\n",
            "\n",
            "Review: I don't know what Dr. Goldberg was like before  moving to Arizona, but let me te...\n",
            "â†’ True: Negative | Predicted: Negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "print(\"Loading Yelp dataset...\")\n",
        "dataset = load_dataset(\"yelp_review_full\", split=\"train[:2%]\")  # Use small subset for speed\n",
        "texts = dataset[\"text\"]\n",
        "labels = dataset[\"label\"]  # 0â€“4 (1 to 5 stars)\n",
        "\n",
        "# Map to sentiment: 0,1,2 = negative; 3,4 = positive\n",
        "sentiments = [\"Negative\" if l < 2 else \"Positive\" for l in labels]\n",
        "\n",
        "\n",
        "print(\"Vectorizing text...\")\n",
        "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
        "X = vectorizer.fit_transform(texts)\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "\n",
        "n_docs, n_words = X.shape\n",
        "n_topics = 100        # ðŸŸ¢ 100 Topics\n",
        "alpha = 0.1           # Dirichlet prior for doc-topic\n",
        "beta = 0.1            # Dirichlet prior for topic-word\n",
        "n_iters = 20          # Lower iterations for faster run\n",
        "\n",
        "\n",
        "word_indices = [X[i].nonzero()[1] for i in range(n_docs)]\n",
        "z = [np.random.randint(0, n_topics, len(wi)) for wi in word_indices]\n",
        "\n",
        "doc_topic = np.zeros((n_docs, n_topics)) + alpha\n",
        "topic_word = np.zeros((n_topics, n_words)) + beta\n",
        "topic_count = np.zeros(n_topics) + n_words * beta\n",
        "\n",
        "# Initialize counts\n",
        "for d, wi in enumerate(word_indices):\n",
        "    for i, w in enumerate(wi):\n",
        "        topic = z[d][i]\n",
        "        doc_topic[d, topic] += 1\n",
        "        topic_word[topic, w] += 1\n",
        "        topic_count[topic] += 1\n",
        "\n",
        "\n",
        "print(f\"Running Gibbs Sampling for {n_topics} topics...\")\n",
        "for it in range(n_iters):\n",
        "    for d, wi in enumerate(word_indices):\n",
        "        for i, w in enumerate(wi):\n",
        "            topic = z[d][i]\n",
        "\n",
        "            # Decrement counts\n",
        "            doc_topic[d, topic] -= 1\n",
        "            topic_word[topic, w] -= 1\n",
        "            topic_count[topic] -= 1\n",
        "\n",
        "            # Conditional probability\n",
        "            p_z = (topic_word[:, w] / topic_count) * doc_topic[d, :]\n",
        "            p_z /= np.sum(p_z)\n",
        "\n",
        "            # Sample new topic\n",
        "            new_topic = np.random.choice(np.arange(n_topics), p=p_z)\n",
        "\n",
        "            # Increment counts\n",
        "            z[d][i] = new_topic\n",
        "            doc_topic[d, new_topic] += 1\n",
        "            topic_word[new_topic, w] += 1\n",
        "            topic_count[new_topic] += 1\n",
        "\n",
        "    print(f\"Iteration {it+1}/{n_iters} complete\")\n",
        "\n",
        "print(f\"\\nâœ… Gibbs sampling complete for {n_topics} topics!\")\n",
        "\n",
        "\n",
        "# Compare predicted vs actual sentiment (based on positive/negative words)\n",
        "positive_words = {\"good\", \"great\", \"amazing\", \"love\", \"excellent\", \"awesome\", \"nice\", \"fantastic\"}\n",
        "negative_words = {\"bad\", \"worst\", \"terrible\", \"poor\", \"awful\", \"boring\", \"disappointing\"}\n",
        "\n",
        "pred_sentiments = []\n",
        "for d, wi in enumerate(word_indices):\n",
        "    doc_words = vocab[wi]\n",
        "    pos = sum(w in positive_words for w in doc_words)\n",
        "    neg = sum(w in negative_words for w in doc_words)\n",
        "    pred_sentiments.append(\"Positive\" if pos >= neg else \"Negative\")\n",
        "\n",
        "\n",
        "correct = sum(p == t for p, t in zip(pred_sentiments, sentiments))\n",
        "accuracy = correct / len(sentiments)\n",
        "\n",
        "print(f\"\\nModel Accuracy (approx): {accuracy:.2f}\")\n",
        "print(\"\\nSample Results:\\n\")\n",
        "for i in range(5):\n",
        "    print(f\"Review: {texts[i][:80]}...\")\n",
        "    print(f\"â†’ True: {sentiments[i]} | Predicted: {pred_sentiments[i]}\\n\")\n"
      ]
    }
  ]
}