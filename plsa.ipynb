{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZTOlKOCuA+uBhiZtarA0w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreevanimtcs2502/sreevanimtcs2502/blob/main/plsa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzTDNwnW83yV",
        "outputId": "4947b7a0-f22c-4058-ee34-c84194b8023c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 001  val_perplexity=707.99\n",
            "Iter 002  val_perplexity=708.20\n",
            "Iter 003  val_perplexity=707.77\n",
            "Iter 004  val_perplexity=706.90\n",
            "Iter 005  val_perplexity=705.67\n",
            "Iter 006  val_perplexity=704.09\n",
            "Iter 007  val_perplexity=702.19\n",
            "Iter 008  val_perplexity=700.01\n",
            "Iter 009  val_perplexity=697.61\n",
            "Iter 010  val_perplexity=695.03\n",
            "Iter 011  val_perplexity=692.33\n",
            "Iter 012  val_perplexity=689.55\n",
            "Iter 013  val_perplexity=686.73\n",
            "Iter 014  val_perplexity=683.90\n",
            "Iter 015  val_perplexity=681.08\n",
            "Iter 016  val_perplexity=678.30\n",
            "Iter 017  val_perplexity=675.57\n",
            "Iter 018  val_perplexity=672.91\n",
            "Iter 019  val_perplexity=670.32\n",
            "Iter 020  val_perplexity=667.83\n",
            "Iter 021  val_perplexity=665.43\n",
            "Iter 022  val_perplexity=663.14\n",
            "Iter 023  val_perplexity=660.94\n",
            "Iter 024  val_perplexity=658.85\n",
            "Iter 025  val_perplexity=656.86\n",
            "Iter 026  val_perplexity=654.97\n",
            "Iter 027  val_perplexity=653.17\n",
            "Iter 028  val_perplexity=651.46\n",
            "Iter 029  val_perplexity=649.83\n",
            "Iter 030  val_perplexity=648.28\n",
            "Iter 031  val_perplexity=646.80\n",
            "Iter 032  val_perplexity=645.40\n",
            "Iter 033  val_perplexity=644.06\n",
            "Iter 034  val_perplexity=642.80\n",
            "Iter 035  val_perplexity=641.59\n",
            "Iter 036  val_perplexity=640.45\n",
            "Iter 037  val_perplexity=639.35\n",
            "Iter 038  val_perplexity=638.31\n",
            "Iter 039  val_perplexity=637.32\n",
            "Iter 040  val_perplexity=636.36\n",
            "Iter 041  val_perplexity=635.45\n",
            "Iter 042  val_perplexity=634.57\n",
            "Iter 043  val_perplexity=633.72\n",
            "Iter 044  val_perplexity=632.91\n",
            "Iter 045  val_perplexity=632.12\n",
            "Iter 046  val_perplexity=631.36\n",
            "Iter 047  val_perplexity=630.63\n",
            "Iter 048  val_perplexity=629.92\n",
            "Iter 049  val_perplexity=629.24\n",
            "Iter 050  val_perplexity=628.58\n",
            "Iter 051  val_perplexity=627.94\n",
            "Iter 052  val_perplexity=627.33\n",
            "Iter 053  val_perplexity=626.73\n",
            "Iter 054  val_perplexity=626.15\n",
            "Iter 055  val_perplexity=625.59\n",
            "Iter 056  val_perplexity=625.04\n",
            "Iter 057  val_perplexity=624.52\n",
            "Iter 058  val_perplexity=624.01\n",
            "Iter 059  val_perplexity=623.52\n",
            "Iter 060  val_perplexity=623.04\n",
            "Iter 061  val_perplexity=622.58\n",
            "Iter 062  val_perplexity=622.14\n",
            "Iter 063  val_perplexity=621.72\n",
            "Iter 064  val_perplexity=621.31\n",
            "Iter 065  val_perplexity=620.92\n",
            "Iter 066  val_perplexity=620.55\n",
            "Iter 067  val_perplexity=620.20\n",
            "Iter 068  val_perplexity=619.86\n",
            "Iter 069  val_perplexity=619.54\n",
            "Iter 070  val_perplexity=619.23\n",
            "Iter 071  val_perplexity=618.94\n",
            "Iter 072  val_perplexity=618.66\n",
            "Iter 073  val_perplexity=618.38\n",
            "Iter 074  val_perplexity=618.12\n",
            "Iter 075  val_perplexity=617.87\n",
            "Iter 076  val_perplexity=617.63\n",
            "Iter 077  val_perplexity=617.39\n",
            "Iter 078  val_perplexity=617.16\n",
            "Iter 079  val_perplexity=616.94\n",
            "Iter 080  val_perplexity=616.73\n",
            "Iter 081  val_perplexity=616.52\n",
            "Iter 082  val_perplexity=616.31\n",
            "Iter 083  val_perplexity=616.11\n",
            "Iter 084  val_perplexity=615.92\n",
            "Iter 085  val_perplexity=615.73\n",
            "Iter 086  val_perplexity=615.55\n",
            "Iter 087  val_perplexity=615.37\n",
            "Iter 088  val_perplexity=615.19\n",
            "Iter 089  val_perplexity=615.02\n",
            "Iter 090  val_perplexity=614.86\n",
            "Iter 091  val_perplexity=614.69\n",
            "Iter 092  val_perplexity=614.54\n",
            "Iter 093  val_perplexity=614.38\n",
            "Iter 094  val_perplexity=614.23\n",
            "Iter 095  val_perplexity=614.09\n",
            "Iter 096  val_perplexity=613.94\n",
            "Iter 097  val_perplexity=613.80\n",
            "Iter 098  val_perplexity=613.67\n",
            "Iter 099  val_perplexity=613.53\n",
            "Iter 100  val_perplexity=613.40\n",
            "\n",
            "Best validation perplexity across restarts: 613.40\n",
            "Train Perplexity (best): 509.22\n",
            "Val   Perplexity (best): 613.40\n",
            "\n",
            "--- Top Words per Topic ---\n",
            "Topic 1: br, just, 10, real, say, scene, away, matter, look, use\n",
            "Topic 2: horror, effects, low, budget, special, killer, scenes, cheap, gore, look\n",
            "Topic 3: like, school, high, kids, little, just, really, 10, girl, don\n",
            "Topic 4: life, father, children, world, music, mother, son, song, musical, child\n",
            "Topic 5: movie, movies, good, seen, watch, saw, dvd, 10, watched, like\n",
            "Topic 6: role, performance, best, actor, play, director, actors, cast, script, played\n",
            "Topic 7: bad, just, acting, really, don, good, worst, time, make, money\n",
            "Topic 8: black, version, new, book, war, men, women, world, story, white\n",
            "Topic 9: characters, series, tv, good, original, time, episode, watch, plot, character\n",
            "Topic 10: just, funny, comedy, like, think, watch, ll, best, don, laugh\n",
            "Topic 11: action, dead, man, good, plot, guy, goes, girl, end, gets\n",
            "Topic 12: story, love, great, character, good, characters, wife, really, wonderful, perfect\n",
            "Topic 13: time, man, mr, does, work, way, wish, dog, home, makes\n",
            "Topic 14: people, like, just, trying, make, things, don, think, way, want\n",
            "Topic 15: film, films, director, scenes, scene, making, time, shot, style, french\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "data = pd.read_csv(f\"{path}/IMDB Dataset.csv\")\n",
        "data = data.sample(3000, random_state=42)\n",
        "texts = data['review'].astype(str).tolist()\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=1500, min_df=2)\n",
        "X_all = vectorizer.fit_transform(texts).toarray()\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "D, V = X_all.shape\n",
        "\n",
        "train_idx, val_idx = train_test_split(np.arange(D), test_size=0.2, random_state=42)\n",
        "X_train = X_all[train_idx]\n",
        "X_val = X_all[val_idx]\n",
        "\n",
        "def perplexity_from_params(X, P_w_z, P_z_d):\n",
        "    total = X.sum()\n",
        "    ll = 0.0\n",
        "    P_w_zT = P_w_z\n",
        "    for d in range(X.shape[0]):\n",
        "        p_w_d = P_z_d[d].dot(P_w_zT)\n",
        "        p_w_d = np.clip(p_w_d, 1e-12, None)\n",
        "        ll += (X[d] * np.log(p_w_d)).sum()\n",
        "    perp = np.exp(-ll / total)\n",
        "    return perp\n",
        "\n",
        "def run_plsi_em(X_train, X_val, K=10, max_iter=100, smoothing=1e-2,\n",
        "                tol=1e-4, patience=5, verbose=True):\n",
        "    D_train, V = X_train.shape\n",
        "    D_val = X_val.shape[0]\n",
        "    rng = np.random.RandomState(None)\n",
        "\n",
        "    P_z = rng.dirichlet(np.ones(K))\n",
        "    P_w_z = rng.dirichlet(np.ones(V), size=K)\n",
        "    P_z_d_train = rng.dirichlet(np.ones(K), size=D_train)\n",
        "\n",
        "    best_state = None\n",
        "    best_val_perp = np.inf\n",
        "    no_improve = 0\n",
        "    history = []\n",
        "\n",
        "    for it in range(1, max_iter+1):\n",
        "        N_k_w = np.zeros((K, V))\n",
        "        N_k_d = np.zeros((K, D_train))\n",
        "        N_k = np.zeros(K)\n",
        "\n",
        "        for d in range(D_train):\n",
        "            x_dw = X_train[d]\n",
        "            nonzero_idx = np.nonzero(x_dw)[0]\n",
        "            if nonzero_idx.size == 0:\n",
        "                continue\n",
        "            pzd = P_z_d_train[d]\n",
        "            Pw = P_w_z[:, nonzero_idx]\n",
        "            numer = (pzd[:, None] * Pw)\n",
        "            denom = numer.sum(axis=0, keepdims=True) + 1e-12\n",
        "            post = numer / denom\n",
        "            counts = x_dw[nonzero_idx]\n",
        "            N_k_w[:, nonzero_idx] += post * counts\n",
        "            N_k_d[:, d] += (post * counts).sum(axis=1)\n",
        "            N_k += (post * counts).sum(axis=1)\n",
        "\n",
        "        P_w_z = (N_k_w + smoothing)\n",
        "        P_w_z /= P_w_z.sum(axis=1, keepdims=True)\n",
        "\n",
        "        P_z_d_train = (N_k_d.T + smoothing)\n",
        "        P_z_d_train /= P_z_d_train.sum(axis=1, keepdims=True)\n",
        "\n",
        "        P_z = (N_k + smoothing)\n",
        "        P_z /= P_z.sum()\n",
        "\n",
        "        P_z_w = (P_w_z * P_z[:, None])\n",
        "        P_z_w /= P_z_w.sum(axis=0, keepdims=True) + 1e-12\n",
        "\n",
        "        P_z_d_val = np.zeros((D_val, K))\n",
        "        for i, d in enumerate(range(X_val.shape[0])):\n",
        "            x = X_val[d]\n",
        "            nz = np.nonzero(x)[0]\n",
        "            if nz.size == 0:\n",
        "                P_z_d_val[i] = 1.0 / K\n",
        "            else:\n",
        "                P_z_d_val[i] = (x[nz] @ P_z_w[:, nz].T)\n",
        "                s = P_z_d_val[i].sum()\n",
        "                if s <= 0:\n",
        "                    P_z_d_val[i] = 1.0 / K\n",
        "                else:\n",
        "                    P_z_d_val[i] /= s\n",
        "\n",
        "        val_perp = perplexity_from_params(X_val, P_w_z, P_z_d_val)\n",
        "        history.append(val_perp)\n",
        "        if verbose:\n",
        "            print(f\"Iter {it:03d}  val_perplexity={val_perp:.2f}\")\n",
        "\n",
        "        if val_perp + tol < best_val_perp:\n",
        "            best_val_perp = val_perp\n",
        "            best_state = (P_z.copy(), P_w_z.copy(), P_z_d_train.copy())\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                if verbose:\n",
        "                    print(f\"No improvement for {patience} iters - stopping early.\")\n",
        "                break\n",
        "\n",
        "    return best_state, best_val_perp, history\n",
        "\n",
        "n_restarts = 1\n",
        "best_overall = None\n",
        "best_perp = np.inf\n",
        "histories = []\n",
        "for r in range(n_restarts):\n",
        "    if r>0:\n",
        "        print(f\"\\nRestart {r+1}/{n_restarts}\")\n",
        "    state, valp, hist = run_plsi_em(X_train, X_val, K=15, max_iter=100,\n",
        "                                   smoothing=0.1, tol=1e-4, patience=7, verbose=True)\n",
        "    histories.append(hist)\n",
        "    if valp < best_perp:\n",
        "        best_perp = valp\n",
        "        best_overall = state\n",
        "\n",
        "print(f\"\\nBest validation perplexity across restarts: {best_perp:.2f}\")\n",
        "P_z_best, P_w_z_best, P_z_d_train_best = best_overall\n",
        "\n",
        "train_perp = perplexity_from_params(X_train, P_w_z_best, P_z_d_train_best)\n",
        "P_z_w = (P_w_z_best * P_z_best[:, None])\n",
        "P_z_w /= P_z_w.sum(axis=0, keepdims=True) + 1e-12\n",
        "P_z_d_val_final = np.zeros((X_val.shape[0], P_z_w.shape[0]))\n",
        "for i in range(X_val.shape[0]):\n",
        "    x = X_val[i]\n",
        "    nz = np.nonzero(x)[0]\n",
        "    if nz.size == 0:\n",
        "        P_z_d_val_final[i] = 1.0 / P_z_w.shape[0]\n",
        "    else:\n",
        "        P_z_d_val_final[i] = (x[nz] @ P_z_w[:, nz].T)\n",
        "        P_z_d_val_final[i] /= P_z_d_val_final[i].sum()\n",
        "val_perp_final = perplexity_from_params(X_val, P_w_z_best, P_z_d_val_final)\n",
        "\n",
        "print(f\"Train Perplexity (best): {train_perp:.2f}\")\n",
        "print(f\"Val   Perplexity (best): {val_perp_final:.2f}\")\n",
        "\n",
        "K = P_w_z_best.shape[0]\n",
        "num_top_words = 10\n",
        "\n",
        "print(\"\\n--- Top Words per Topic ---\")\n",
        "for k in range(K):\n",
        "    top_word_indices = P_w_z_best[k, :].argsort()[-num_top_words:][::-1]\n",
        "    top_words = [vocab[idx] for idx in top_word_indices]\n",
        "    print(f\"Topic {k+1}: {', '.join(top_words)}\")"
      ]
    }
  ]
}